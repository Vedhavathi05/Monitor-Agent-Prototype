# TV Monitor Agent â€“ Live Feed Transcription & Summarization

## Overview

This project is a **Monitor Agent prototype** that continuously monitors a live TV stream, extracts audio, generates **1â€‘minute transcripts**, and produces **short AI summaries** for each segment. The system is designed to work **near realâ€‘time**, with robustness against local resource limitations.

The prototype was built as part of a technical assignment to demonstrate:

* Live stream processing
* Audio extraction
* Speechâ€‘toâ€‘text transcription
* AIâ€‘assisted summarization
* A simple frontend for monitoring and control

---

## Features

* ğŸ¥ Live TV feed ingestion using **FFmpeg**
* ğŸ”Š Continuous audio extraction
* ğŸ“ Transcription every **1 minute** using **OpenAI Whisper (local)**
* ğŸ§  AI summarization (â‰¤ 15 words per segment)

  * Local LLM via **Ollama (phi model)**
  * Automatic **fallback extractive summary** if LLM fails
* â¯ï¸ Start / Stop monitoring controls
* ğŸ“Š Frontend dashboard showing:

  * Timestamp
  * Video time window
  * Full transcript
  * AI summary

---

## Tech Stack

### Backend

* Python 3.11
* FastAPI
* Uvicorn
* FFmpeg
* Whisper (speechâ€‘toâ€‘text)
* Ollama (local LLM)

### Frontend

* React (Vite)
* Plain CSS

---

## Project Structure

```
AI Summarizer/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py          # FastAPI app & monitor loop
â”‚   â”œâ”€â”€ capture.py       # Live stream audio capture (FFmpeg)
â”‚   â”œâ”€â”€ transcribe.py    # Whisper transcription logic
â”‚   â”œâ”€â”€ processor.py    # AI summarization + fallback logic
â”‚   â”œâ”€â”€ storage.py      # Inâ€‘memory storage for records
â”‚   â”œâ”€â”€ config.py       # Stream & timing configuration
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â”œâ”€â”€ App.css
â”‚   â”‚   â””â”€â”€ main.jsx
â”‚   â”œâ”€â”€ index.html
â”‚   â””â”€â”€ package.json
â”‚
â””â”€â”€ README.md
```

---

## Setup Instructions

### 1ï¸âƒ£ Prerequisites

* Python **3.11** installed
* Node.js **18+** installed
* FFmpeg installed and added to PATH
* Ollama installed

Verify installations:

```bash
python --version
node --version
ffmpeg -version
```

---

### 2ï¸âƒ£ Backend Setup

```bash
cd backend
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
```

Start the backend:

```bash
python -m uvicorn main:app --reload
```

Backend runs at:

```
http://127.0.0.1:8000
```

---

### 3ï¸âƒ£ Ollama Setup (Local LLM)

Pull the model:

```bash
ollama pull phi
```

Verify:

```bash
ollama list
```

The project uses:

```
phi:latest
```

---

### 4ï¸âƒ£ Frontend Setup

```bash
cd frontend
npm install
npm run dev
```

Frontend runs at:

```
http://localhost:5173
```

---

## How It Works

1. User clicks **Start Monitoring**
2. Backend starts a background thread
3. Every 1 minute:

   * Audio is extracted from the live stream
   * Whisper generates a transcript
   * Transcript is summarized:

     * First via local LLM (Ollama)
     * If LLM fails â†’ deterministic extractive fallback
4. Result is stored and exposed via `/data`
5. Frontend polls `/data` and updates the UI

---

## Reliability Design

* **Input truncation** prevents LLM crashes on long transcripts
* **Fallback summarization** guarantees output even if LLM fails
* Guard against multiple monitoring threads
* Designed for CPUâ€‘only environments

This ensures the system never breaks during live monitoring.

---

## Performance Notes

* Processing is done in **fixed 1â€‘minute windows**
* Small delay (â‰ˆ 1 minute) is expected by design
* Optimized for stability rather than lowâ€‘latency streaming

---

## Known Limitations

* Local LLM performance depends on CPU resources
* Not intended for highâ€‘throughput production use
* Inâ€‘memory storage (data resets on restart)

---

## Future Improvements

* Persistent database storage
* GPUâ€‘accelerated inference
* Multiâ€‘channel stream support
* Keyword alerts and topic detection
* Authentication and user management

---

## Summary

This prototype demonstrates an endâ€‘toâ€‘end **live media monitoring system** with realâ€‘time transcription and AI summarization, focusing on **robustness, clarity, and practical engineering tradeâ€‘offs**.

---

**Author:** Vedhavathi Kosuri
